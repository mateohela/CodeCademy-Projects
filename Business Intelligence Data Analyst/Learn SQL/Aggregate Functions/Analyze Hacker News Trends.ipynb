{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735fd8e2",
   "metadata": {},
   "source": [
    "# Analyze Hacker News Trends\n",
    "\n",
    "## The project details:\n",
    "\n",
    "Hacker News is a popular website run by Y Combinator. It’s widely known by people in the tech industry as a community site for sharing news, showing off projects, asking questions, among other things.\n",
    "\n",
    "In this project, you will be working with a table named hacker_news that contains stories from Hacker News since its launch in 2007. It has the following columns:\n",
    "\n",
    "title: the title of the story\n",
    "user: the user who submitted the story\n",
    "score: the score of the story\n",
    "timestamp: the time of the story\n",
    "url: the link of the story\n",
    "This data was kindly made publicly available under the MIT license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- 1 Start by getting a feel for the hacker_news table!\n",
    "-- Let’s find the most popular Hacker News stories:\n",
    "SELECT * FROM hacker_news LIMIT 5;\n",
    "\n",
    "-- Result\n",
    "\n",
    "Query Results\n",
    "title\tuser\tscore\ttimestamp\turl\n",
    "Penny Arcade – Surface Pro 3 update\tvxNsr\t517\t2014-06-16T18:38:48Z\thttp://penny-arcade.com/news/post/2014/06/16/surface-pro-3-update\n",
    "Hacking The Status Game\tamirkhella\t309\t2011-04-18T19:10:06Z\thttp://blog.amirkhella.com/2011/04/18/the-status-game/\n",
    "Postgres CLI with autocompletion and syntax highlighting\tdmmalam\t304\t2015-07-23T16:32:39Z\thttps://github.com/dbcli/pgcli\n",
    "Stephen Fry hits out at ‘infantile’ culture of trigger words and safe spaces\tmetafunctor\t282\t2016-04-12T07:28:16Z\thttp://attitude.co.uk/stephen-fry-hits-out-at-infantile-culture-of-trigger-words-and-safe-spaces/\n",
    "Reversal: Australian Govt picks ODF doc standard over Microsoft\trenai_lemay\t191\t2013-05-29T06:37:40Z\thttp://delimiter.com.au/2013/05/29/reversal-australian-govt-picks-odf-doc-standard/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b913c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- 2 Recent studies have found that online forums tend to be dominated by a small percentage of their users (1-9-90 Rule).\n",
    "-- Is this true of Hacker News?\n",
    "-- Is a small percentage of Hacker News submitters taking the majority of the points?\n",
    "-- First, find the total score of all the stories.\n",
    "SELECT SUM(score) from hacker_news;\n",
    "\n",
    "-- Result\n",
    "\n",
    "Query Results\n",
    "SUM(score)\n",
    "6366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- 3 Next, we need to pinpoint the users who have accumulated a lot of points across their stories.\n",
    "-- Find the individual users who have gotten combined scores of more than 200, and their combined scores.\n",
    "SELECT user, SUM(score) FROM hacker_news GROUP BY user HAVING SUM(score) > 200 ORDER BY 2 DESC;\n",
    "\n",
    "-- Result\n",
    "\n",
    "Query Results\n",
    "user\tSUM(score)\n",
    "vxNsr\t517\n",
    "amirkhella\t309\n",
    "dmmalam\t304\n",
    "metafunctor\t282\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- 4 Then, we want to add these users’ scores together and divide by the total to get the percentage.\n",
    "-- Add their scores together and divide it by the total sum. Like so:\n",
    "-- SELECT (1.0 + 2.0 + 3.0) / 6.0;\n",
    "-- So, is Hacker News dominated by these users?\n",
    "SELECT (517 + 309 + 304 + 282) / 6366.0;\n",
    "\n",
    "-- Result\n",
    "\n",
    "Query Results\n",
    "(517 + 309 + 304 + 282) / 6366.0\n",
    "0.221803330191643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- 5 Oh no! While we are looking at the power users, some users are rickrolling — tricking readers into clicking on a link to a funny video and claiming that it links to information about coding.\n",
    "-- The url of the video is:\n",
    "-- https://www.youtube.com/watch?v=dQw4w9WgXcQ\n",
    "-- How many times has each offending user posted this link?\n",
    "SELECT user, COUNT(*) FROM hacker_news WHERE url LIKE '%watch?v=dQw4w9WgXcQ%' GROUP BY user ORDER BY 2 DESC;\n",
    "\n",
    "-- Result\n",
    "\n",
    "Query Results\n",
    "user\tCOUNT(*)\n",
    "sonnynomnom\t2\n",
    "scorpiosister\t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b17574",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- 6 Hacker News stories are essentially links that take users to other websites.\n",
    "-- Which of these sites feed Hacker News the most:\n",
    "-- GitHub, Medium, or New York Times?\n",
    "-- First, we want to categorize each story based on their source.\n",
    "\n",
    "-- 7 Build on the previous query:\n",
    "-- Add a column for the number of stories from each URL using COUNT().\n",
    "-- Also, GROUP BY the CASE statement.\n",
    "-- Remember that you can refer to a column in GROUP BY using a number.\n",
    "SELECT CASE\n",
    "  WHEN url LIKE '%github.com%' THEN 'GitHub'\n",
    "  WHEN url LIKE '%medium.com%' THEN 'Medium'\n",
    "  WHEN url LIKE '%nytimes.com%' THEN 'New York Times'\n",
    "  ELSE 'Other'\n",
    "  END AS 'Source',\n",
    "  COUNT(*)\n",
    "FROM hacker_news GROUP BY 1 ORDER BY 2 DESC;\n",
    "\n",
    "-- Result\n",
    "\n",
    "Query Results\n",
    "Source\tCOUNT(*)\n",
    "Other\t3952\n",
    "GitHub\t23\n",
    "New York Times\t13\n",
    "Medium\t12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21eac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- 8 Every submitter wants their story to get a high score so that the story makes it to the front page, but…\n",
    "-- What’s the best time of the day to post a story on Hacker News?\n",
    "-- Before we get started, let’s run this query and take a look at the timestamp column:\n",
    "SELECT timestamp FROM hacker_news LIMIT 10;\n",
    "\n",
    "-- Result\n",
    "\n",
    "Query Results\n",
    "timestamp\n",
    "2014-01-27T17:31:13Z\n",
    "2011-10-23T18:46:40Z\n",
    "2016-02-28T06:26:56Z\n",
    "2014-08-12T22:13:10Z\n",
    "2013-03-06T12:28:02Z\n",
    "2011-04-16T21:04:23Z\n",
    "2014-03-18T21:44:46Z\n",
    "2012-11-19T11:54:38Z\n",
    "2016-11-04T13:55:30Z\n",
    "2016-07-02T22:54:47Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f8472",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- 9 SQLite comes with a strftime() function - a very powerful function that allows you to return a formatted date.\n",
    "-- It takes two arguments:\n",
    "-- strftime(format, column)\n",
    "-- Let’s test this function out:\n",
    "SELECT timestamp, strftime('%H', timestamp) FROM hacker_news GROUP BY 1\n",
    "LIMIT 20;\n",
    "\n",
    "-- Result\n",
    "\n",
    "Query Results\n",
    "timestamp\tstrftime('%H', timestamp)\n",
    "2007-03-16T20:52:19Z\t20\n",
    "2007-04-03T03:04:09Z\t03\n",
    "2007-05-01T03:11:17Z\t03\n",
    "2007-05-05T05:43:58Z\t05\n",
    "2007-05-11T05:48:53Z\t05\n",
    "2007-05-25T22:07:18Z\t22\n",
    "2007-06-08T08:44:50Z\t08\n",
    "2007-07-01T00:06:57Z\t00\n",
    "2007-07-27T16:47:00Z\t16\n",
    "2007-08-08T00:45:36Z\t00\n",
    "2007-08-12T07:35:25Z\t07\n",
    "2007-08-16T01:02:49Z\t01\n",
    "2007-08-24T18:17:32Z\t18\n",
    "2007-08-27T17:32:10Z\t17\n",
    "2007-09-11T16:27:49Z\t16\n",
    "2007-09-20T20:31:21Z\t20\n",
    "2007-09-22T08:21:17Z\t08\n",
    "2007-09-26T05:03:17Z\t05\n",
    "2007-10-08T21:28:06Z\t21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- 10 Okay, now we understand how strftime() works. Let’s write a query that returns three columns:\n",
    "-- The hours of the timestamp\n",
    "-- The average score for each hour\n",
    "-- The count of stories for each hour\n",
    "SELECT strftime('%H', timestamp), AVG(score), COUNT(*) FROM hacker_news GROUP BY 1 ORDER BY 2 DESC;\n",
    "\n",
    "-- Result\n",
    "\n",
    "Query Results\n",
    "strftime('%H', timestamp)\tAVG(score)\tCOUNT(*)\n",
    "18\t27.0277777777778\t266\n",
    "07\t21.3333333333333\t104\n",
    "19\t20.4137931034483\t238\n",
    "20\t18.2758620689655\t239\n",
    "12\t14.3103448275862\t123\n",
    "09\t11.125\t119\n",
    "16\t9.75555555555556\t237\n",
    "15\t9.49122807017544\t268\n",
    "17\t8.86046511627907\t215\n",
    "23\t8.43478260869565\t170\n",
    "11\t8.32258064516129\t113\n",
    "06\t7.5\t110\n",
    "03\t6.91304347826087\t134\n",
    "13\t5.67647058823529\t164\n",
    "00\t5.09090909090909\t137\n",
    "01\t4.40740740740741\t146\n",
    "04\t3.875\t116\n",
    "05\t3.55172413793103\t125\n",
    "14\t3.51515151515152\t213\n",
    "21\t3.25\t234\n",
    "22\t3.20833333333333\t185\n",
    "10\t3.07894736842105\t112\n",
    "02\t2.45\t136\n",
    "08\t2.04166666666667\t90\n",
    "6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f25910",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- 11 Let’s edit a few things in the previous query:\n",
    "-- Round the average scores (ROUND()).\n",
    "-- Rename the columns to make it more readable (AS).\n",
    "-- Add a WHERE clause to filter out the NULL values in timestamp.\n",
    "-- Take a look at the result again:\n",
    "-- What are the best hours to post a story on Hacker News?\n",
    "SELECT strftime('%H', timestamp) AS 'Hour', ROUND(AVG(score)) AS 'Average Score', COUNT(*) AS 'Number of Stories' FROM hacker_news WHERE timestamp NOT NULL GROUP BY 1 ORDER BY 2 DESC;\n",
    "\n",
    "-- Result\n",
    "\n",
    "Query Results\n",
    "Hour\tAverage Score\tNumber of Stories\n",
    "18\t27.0\t266\n",
    "07\t21.0\t104\n",
    "19\t20.0\t238\n",
    "20\t18.0\t239\n",
    "12\t14.0\t123\n",
    "09\t11.0\t119\n",
    "16\t10.0\t237\n",
    "17\t9.0\t215\n",
    "15\t9.0\t268\n",
    "23\t8.0\t170\n",
    "11\t8.0\t113\n",
    "06\t8.0\t110\n",
    "03\t7.0\t134\n",
    "13\t6.0\t164\n",
    "00\t5.0\t137\n",
    "14\t4.0\t213\n",
    "05\t4.0\t125\n",
    "04\t4.0\t116\n",
    "01\t4.0\t146\n",
    "22\t3.0\t185\n",
    "21\t3.0\t234\n",
    "10\t3.0\t112\n",
    "08\t2.0\t90\n",
    "02\t2.0\t136"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d80010f",
   "metadata": {},
   "source": [
    "## Review\n",
    "I really like this small database since it gives the opportunity to work with time. Most of the practical Business Analysis projects include time, be it for checking the difference of sales or users engaging with the organisation. All of them need to be within a certain timeframe. About the database, although it is okay with processing it in SQL, I think it would be better if it was processed with python where we can use NLP libraries and we could get more information. For example categorising the titles. There we can see which category has the most articles, which category has the most scores, which category has a stable release. After making a diagram with the stable release, we can check it against others so we can infer some kind of new trend (example: more cybersecurity articles due to a new virus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
